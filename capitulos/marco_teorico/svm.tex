\subsection{SVM}

\paragraph{}Una SVM es un algoritmo o clasificador de aprendizaje automático supervisado utilizado fundamentalmente para problemas de clasificación.
Dado un problema de aprendizaje automático supervisado de clasificación donde las instancias del problema pueden ser clasificadas en $N$ clases y un conjunto de ejemplos de entrenamiento de dicho problema, una SVM busca encontrar un hiperplano que los divida de manera lineal esas $N$ clases. De esta manera, frente a una nueva instancia del problema, la SVM será capaz de clasificarla generando una correspondencia entre esta instancia y una de las $N$ clases posibles.

\paragraph{}Se denominan vectores de soporte a aquellos ejemplos de entrenamiento más cercanos al hiperplano, la distancia entre el hiperplano y un vector de soporte se conoce como margen y para un conjunto de ejemplos de entrenamiento, SVM busca dividirlos de manera óptima con un hiperplano donde se maximice estos márgenes para cada uno de los vectores de soporte, cada uno asociado a su vez a una de las posibles clases de clasificación del problema. En un problema donde las instancias pueden ser clasificadas en dos clases, el hiperplano de dos dimensiones es representado por una línea recta y los vectores de soporte son aquellos ejemplos de entrenamiento más cercanos a ella, desde las dos direcciones posibles. La figura \ref{fig:svm_margin} muestra lo antedicho.

\textsc{\begin{figure}[ht!]
	\centering
    \includegraphics[width=.5\linewidth]{imagenes/svm_margenes.png}
	\caption{En esta figura se observan dos conjuntos de elementos (cuadrados y círculos), clasificados en dos clases, en base a dos características, \textit{x} e \textit{y}. El hiperplano generado por svm se muestra como la recta negra y se marcan los márgenes en lineas punteadas. Así, en color rojo se muestran los vectores de soporte, uno perteneciente a cada clase, siendo z la distancia entre cada vector de soporte y el hiperplano.}
	\label{fig:svm_margin}
\end{figure}}


\paragraph{}El hiperplano estará dado por la ecuación $g(\vec{x}) = \vec{w}^T\vec{x} + w_0$ donde $\vec{w}$ es un vector de pesos y $\vec{x}$ es el vector de características. La distancia a uno de los márgenes está dada por $z = \abs{g(\vec{x})} / \norm{\vec{w}} = 1 / \norm{\vec{w}}$, por lo que el margen total se computa como $1 / \norm{\vec{w}} +  1 / \norm{\vec{w}} = 2 / \norm{\vec{w}}$, así, minimizando la expresión $\norm{\vec{w}}$ se maximizan los márgenes, maximizando la separabilidad de las clases. 

\paragraph{}La confianza en la clasificación de una instancia del problema, estará dada por la distancia de dicho instancia al hiperplano de manera directamente proporcional.
La obtención de un hiperplano que divida de manera lineal a los ejemplos de entrenamiento no siempre es posible. Cuando no es posible se suele utilizar una técnica conocida como \textit{kernelización} para aumentar la dimensión del dominio donde se está buscando un hiperplano e, intentar obtener un hiperplano que separe linealmente a los ejemplos de entrenamiento en esa dimensión. De esta manera, es posible clasificar a nuevas instancias del problema de acuerdo a este nuevo hiperplano y transformar la clasificación obtenida a la dimensión original del problema dada por las clases de clasificación disponibles.

\paragraph{}Cuando se tienen más de dos clases objetivo para la clasificación, se utiliza un SVM Multiclase en el cual la técnica de prendizaje admite dos posibles estrategias. Por un lado \textit{One vs. Rest} (\textit{OVR}) y por otro \textit{One vs. One} (\textit{OVO}). Dado un conjunto de $N$ clases $\{c_1,c_2,\dots,c_N\}$, \textit{OVR} intentará clasificar $c_i$ contra $\{c_1,c_2,\dots,c_{i-1},c_{i+1},\dots,c_{N - 1}\}$ dejando como representantes de la clase $C_N$ los datos que no fueron clasificado en las otras clases. Por otro lado textit{OVO}, intentará clasficar cada clase $i$ contra cada $C_j \forall j \neq i$, creando un hiperplano para cada una de las combinaciones. Comparando las dos estategias, \textit{OVR} genera $C_{N-1}$ clasificadores, en contraste con \textit{OVO} que generará una combinación de $N$ tomada de a dos, generando un número mayor de clasificadores que \textit{OVR}, por otro lado \textit{OVR} es más sensible al no balance de los datos, en el sentido de que si intento clasificar  y la cantidad de datos representantes de esta clase es baja en comparación con el total de datos, esta clase podría verse eclipsada por las demás, en cambio con \textit{OVO} este fenómeno no se manifiesta en igual medida, dado que se generan clasificadores uno a uno. La figura \ref{fig:ovoovr} muestra lo antedicho.

\textsc{\begin{figure}[ht!]
	\centering
    \includegraphics[width=.5\linewidth]{imagenes/ovoovr.png}
	\caption{A la izquierda se muestra la estrategia de entrenamiendo de SVM \textit{OVR}, en la cual cada una de las clases se intenta clasificar contra el resto de las clases. A la derecha se muestra la estrategia \textit{OVO}, que genera clasificadores para cada par de clases.}
	\label{fig:ovoovr}
\end{figure}}

\paragraph{}SVM utiliza lo que se conoce como \textit{Kernel Trick}, un método que implica agregar dimensiones mediante parametros de \textit{Kernel}, a los datos que no separables por hiperplanos para lograr clasificarlos, lo que le brinda flexibilidad a la hora de clasificar datos complejos. Así también, como resultado de una selección imprudente de \textit{Kernel Trick} puede llevar a sobreajuste del modelo.

\paragraph{}Entre las desventajas de utilizar SVM encontramos que los tiempos de entrenamiento asociados pueden ser significativos para grandes conjuntos de entrenamiento, pudiendo tener consumos no menores de memoria tanto en entrenamiento como en pruebas. Así también la selección de parámetros de \textit{Kernel} para el realizar el \textit{Kernel Trick} puede ser complejo y se debe hacer evitando el sobreajuste.