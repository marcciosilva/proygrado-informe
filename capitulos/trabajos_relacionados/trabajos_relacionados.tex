\chapter{Trabajos relacionados} \label{section-trabajo-relacionado}
\markright{Trabajos relacionados}

\paragraph{}En este capítulo se describen los principales trabajos relacionados a la temática abordada en este proyecto.
El paradigma de SV ha sido explorado únicamente en dos trabajos \cite{savant-original} \cite{savant-bag}, que son abordados en la Sección \ref{section-trabajos-relacionados-savant}.
La Sección \ref{section-trabajos-tecnicas-clasificacion} está dedicada a técnicas de clasificación en aprendizaje supervisado, mientras que la Sección \ref{section-trabajos-seleccion-extraccion-atributos} trata la temática de selección y extracción de atributos en aprendizaje automático.
Finalmente, en la Sección \ref{section-trabajos-relacionados-mapreduce} se aborda el modelo de programación MapReduce, que permite la ejecución paralela en un sistema implementado bajo el paradigma de SV.

% ############### RESEÑA ###############

\section{Savant Virtual} \label{section-trabajos-relacionados-savant}

% FIRST
% citep para que aparezca algo tipo Dorronsoro et al [1]
% cite o citet para que aparezca sólo la referencia tipo [1]
\paragraph{} El síndrome de Savant es una condición donde los individuos que la padecen, conocidos como savants, son capaces de resolver tareas de cálculo, memoria o mnemotecnia, como encontrar el día de la semana para una fecha dada o listar números primos utilizando \textit{métodos desconocidos}, en tiempos inferiores a los esperados normalmente y con una gran precisión. 

\paragraph{}Tomando como inspiración al síndrome de Savant, \citet{savant-original} introdujeron el paradigma de SV con el objetivo de generar programas que, utilizando aprendizaje automático supervisado, encuentren soluciones a problemas con una calidad comparable a las soluciones ofrecidas por otros algoritmos conocidos.
De esta manera, el componente de aprendizaje automático está asociado de manera simbólica a las reglas que un Savant incorpora automáticamente.
Los autores aplicaron el paradigma de SV sobre el problema HCSP, presentado en la Sección \ref{section-descripcion-problema}, utilizando también a Min-Min como criterio de referencia para llevar a cabo el aprendizaje automático.
Con el objetivo de aplicar SV a la resolución de este problema de manera paralela, el sistema planteado implementó el modelo MapReduce.
Bajo este modelo, cada \textit{mapper} se corresponde con un clasificador multiclase entrenado previamente (utilizando SVM). Un único \textit{reducer} realiza una búsqueda local con el objetivo de mejorar la solución y posteriormente devuelve su solución.
El sistema permite utilizar tantos \textit{mappers} como tareas se tengan en la instancia del problema, posibilitando la ejecución en paralelo de cada uno.
Por otra parte, cada clasificador del sistema se entrena utilizando las soluciones obtenidas mediante la aplicación del algoritmo Min-Min.
Finalmente, se consigue un sistema que puede trabajar de manera paralela, generando soluciones basadas en un entrenamiento realizado utilizando el criterio Min-Min como referencia. 

Los autores evaluaron el paradigma propuesto sobre un conjunto de instancias de prueba generadas de forma aleatoria (en la forma de matrices ETC, como fue presentado en la sección \ref{section:descripcion-problema,subsection:estimacion-tiempo-ejecucion}), con el objetivo de comparar las soluciones obtenidas mediante Savant con las de Min-Min.
Cada instancia de prueba estaba compuesta por un conjunto de tareas, un conjunto de máquinas y las duraciones de las tareas para cada máquina. 
% TODO tal vez dar un poco más de detalle del estudio, tipo decir cuántas instancias se usaron
Los resultados mostraron una precisión promedio (similaridad con la solución de referencia) del 82\%, resultando mejor la precisión para instancias más pequeñas del problema (128 tareas y 4 máquinas) que para instancias de mayor tamaño (512 tareas y 16 máquinas). 
% TODO ¿poner dimensiones como $128 \times 4$ ya que esta notación es explicada en la parte de descripción del problema?
Fueron obtenidos mejores resultados al utilizar un \textit{reducer} con búsqueda local frente a un \textit{reducer} simple que genera una solución en base a la salida directa de cada \textit{mapper} sin hacer modificaciones.
En algunas instancias de prueba SV fue capaz de encontrar mejores soluciones que Min-Min. 

El trabajo de \citet{savant-original} presenta oportunidades de trabajo futuro desde diversos puntos de vista; el \textit{reducer} puede llevar a cabo una búsqueda paralela de mejores soluciones, se puede estudiar el uso de otro tipo de clasificadores y se pueden estudiar aplicaciones para verificar la adaptabilidad de SV a otros problemas y comparar su desempeño frente al obtenido para el problema HCSP. 

Este trabajo representó el punto de partida para la investigación en torno a SV y proporciona el marco de trabajo que se sigue en el proyecto de grado, dado que involucra el mismo modelo conceptual, tipos de pruebas y modelado de problemas.

% SECOND
\paragraph{}En \citet{savant-bag} se estudió la aplicación de SV para la generación automática de programas para resolver el problema de la mochila.
Evaluar SV para el problema de la mochila resulta interesante porque representa un problema de optimización combinatoria NP-difícil y además es un tipo de problema diferente al estudiado en el trabajo previo (HCSP) en términos de sus variables binarias y con restricciones simples, en vez de enteras y sin restricciones.
En este problema se tiene un contenedor con una capacidad $W$ y un conjunto $E$ de $n$ objetos, cada uno con un beneficio $p_i$ y un peso $w_i$ que no excede a $W$.
El objetivo perseguido al resolver este problema es el de seleccionar objetos de $E$ a introducir en el contenedor, maximizando el beneficio que estos aportan, sujeto a que la suma de los pesos de los objetos introducidos no exceda la capacidad $W$.
Con respecto al modelado del problema, se optó por representar a una instancia con un peso asociado a la capacidad del contenedor estudiado y un vector donde cada índice representa a un objeto con su peso y su beneficio asociado.
Se aplicó SV respetando el modelo planteado en \citet{savant-original}, utilizando también SVM como clasificador y mapper.
En particular, se utilizaron tantos \textit{mappers} (replicados) como objetos disponibles en la instancia del problema y la clasificación de cada \textit{mapper} indicaba si ese objeto había sido incluido o no en la selección de objetos asignados al contenedor.
Con respecto al \textit{reducer}, se aplicó una búsqueda local aleatorizada a la hora de generar la solución frente a una instancia del problema y dos mecanismos de corrección para soluciones infactibles: corrección ávida por beneficio y corrección ávida por peso.
La corrección ávida por beneficio se basa en eliminar sucesivamente el objeto que aporte el menor beneficio hasta llegar a una solución factible, mientras que la corrección ávida por peso se basa en buscar al objeto de menor peso que tenga un peso mayor o igual al sobrepeso de la solución y eliminarlo, o eliminar al objeto de mayor peso en caso de no encontrarse objetos que cumplan con la condición anterior.
Durante el trabajo se utilizaron 15.750 instancias del problema (en varios conjuntos de datos), de entre 100 y 1.500 objetos, con correlación de peso y beneficio variable para cada tamaño del problema.
Con el fin de determinar una selección de atributos adecuada (dado que se contaba con información de peso y beneficio para cada objeto y además la capacidad del contenedor), se estudió la precisión media (porcentaje de soluciones correctas retornadas por la SVM) para cada conjunto de datos disponible, utilizando una selección diferente en cada instancia del estudio para el entrenamiento.
Dado que la precisión media para todas las configuraciones fue elevada y hubieron diferencias pequeñas entre las configuraciones con respecto a este valor, se optó por emplear al tipo de selección más sencillo y directo, que resultó ser el peso del objeto, su beneficio y la capacidad de la mochila.
Experimentalmente se determinó que al utilizar más del 15\% de los datos para entrenar al clasificador las mejoras en precisión resultaron marginales, por lo que se optó por utilizar únicamente ese porcentaje de los datos, reduciendo de esta manera los tiempos de entrenamiento de la SVM.
Como resultado se obtuvo un error medio del 3,2\% con respecto al resultado óptimo para cada instancia del problema, obtenido mediante el uso de un algoritmo exacto de referencia.
El valor del trabajo de \citet{savant-bag} para este proyecto de grado reside en el hecho de que amplía la información disponible en relación a SV.

% ############### RESEÑA ###############
\section{Técnicas de clasificación} \label{section-trabajos-tecnicas-clasificacion}

\paragraph{}El trabajo de \citet{ml-survey} presenta una reseña sobre métodos de aprendizaje automático, principalmente de carácter expositivo, que ofrece guías acerca de cómo seleccionar apropiadamente clasificadores de aprendizaje automático para casos de uso comúnmente encontrados en la práctica.
Se presenta a los árboles de decisión, que ofrecen buenos resultados para problemas donde los atributos toman valores discretos o categóricos; por lo tanto no sería acertado aplicarlos para un problema de optimización combinatoria con variables numéricas, como es el caso del problema estudiado en este proyecto de grado.
Se destaca el hecho de que los algoritmos estadísticos en general resultan menos precisos que aquellos más sofisticados como las redes neuronales artificiales, aunque tienen tiempos de aprendizaje menores.
Los algoritmos como las redes neuronales artificiales o SVM requieren de un mayor ajuste de parámetros, no son modelos interpretables por humanos y requieren de un gran conjunto de datos de entrenamiento para ser precisos al clasificar.
Por otra parte, se presentan algoritmos lógicos como el de \textit{k-nearest neighbors} o \textit{KNN}, que tienen parámetros sencillos de ajustar y son transparentes en términos de su funcionamiento, pero que involucran una carga de memoria poco conveniente a la hora de estudiar problemas de grandes dimensiones.

\newpage % orphaned line.

El valor del trabajo de \citet{ml-survey} para este proyecto de grado reside en la guía comparativa de selección de métodos de aprendizaje automático que ofrece, que presenta los algoritmos más comunes para cada variante de aprendizaje automático, como \textit{ID3} para árboles de decisión y \textit{Backpropagation} para redes neuronales.

% ############### RESEÑA ###############

\section{Selección y extracción de atributos} \label{section-trabajos-seleccion-extraccion-atributos}

\paragraph{} El trabajo de \citet{fs-survey} ofrece información acerca de las técnicas más utilizadas para la selección de atributos en aprendizaje automático.
La selección de atributos refiere a la acción de seleccionar un subconjunto de atributos o \textit{features} de las instancias del problema a resolver, que efectivamente puedan describir al problema reduciendo el efecto del ruido o variables irrelevantes.
Además, al reducir la cantidad de variables que describen un problema es posible comprender los datos de mejor manera y reducir los requerimientos computacionales asociados a su manejo.
El problema de seleccionar atributos resulta no trivial dado que si las instancias de entrenamiento utilizadas tienen $N$ atributos posibles, es necesario tomar en cuenta $2^N$ subconjuntos posibles de atributos.
\citet{fs-survey} presentan los siguientes tipos de métodos de selección de atributos para aprendizaje supervisado:
\begin{itemize}
\item \textit{Filter}: Ordena a los atributos de acuerdo a algún criterio de ordenamiento que les asigna un valor y elimina aquellos atributos que queden por debajo de un umbral mínimo.
Se sugieren algoritmos para la determinación de la relevancia de cada atributo, siendo el algoritmo \textit{Mutual Information} el más relevante, que mediante el cálculo de la entropía condicional entre dos atributos determina qué tan dependientes son entre sí, lo que permite determinar qué tan dependiente es un atributo de la clasificación de una instancia de entrenamiento.
\item \textit{Wrapper}: Se utilizan algoritmos de búsqueda para encontrar un subconjunto de atributos de manera heurística, no exacta.
Cada subconjunto se evalúa construyendo un clasificador entrenado con los datos sesgados utilizando únicamente los atributos del subconjunto de atributos obtenido y evaluando su precisión con respecto a un conjunto de validación.
Este tipo de método de selección resulta muy costoso en términos computacionales, especialmente si el conjunto de datos es muy extenso.
\item \textit{Embedded}: Este tipo de métodos surgió como respuesta a los problemas de eficiencia asociados a los métodos de tipo \textit{Wrapper}.
Utilizan una alternativa para la evaluación, dada por una función objetivo basada en la entropía condicional entre los atributos del subconjunto de atributos actual y la clasificación.
Este tipo de método de selección reduce el tiempo computacional requerido por métodos de tipo \textit{Wrapper}.
\end{itemize}
\paragraph{}A pesar de que cada algoritmo de selección de atributos puede comportarse de manera diferente de acuerdo al tipo de datos utilizado, \citet{fs-survey} llevaron a cabo un experimento para mostrar la diferencia entre utilizar selección de atributos y no hacerlo.
En dicho experimento se generaron conjuntos de datos, cada uno correspondiente a una condición médica, teniendo potencialmente distintos atributos.
Cada instancia de entrenamiento constaba de entradas asociadas a señales eléctricas provenientes de un sistema médico y una salida o clasificación que indicaba a qué condición médica correspondían esas entradas (cáncer, diabetes, etc.).
Para cada conjunto de datos se destinó el 50\% al entrenamiento y el 50\% a la validación y se consideró como medida de rendimiento la precisión de cada clasificador sobre el conjunto de validación.
Se encontró que, en general, la precisión de los clasificadores que usaban selección de atributos (reduciendo la cantidad de atributos a la mitad) se mantenía muy próxima a la de los clasificadores que utilizaban todos los atributos.
Por ejemplo, para un conjunto de datos que determinaba si un paciente tenía cáncer de acuerdo a 10 atributos, un clasificador (en particular SVM) obtuvo una precisión mayor al 96\%, mientras que el mismo clasificador entrenado con 5 atributos mantuvo una precisión del 95\%.
Este trabajo resultó de gran importancia para determinar cómo llevar a cabo la selección de atributos para la representación del problema estudiado en este proyecto de grado.

% SECOND
\paragraph{}El trabajo de \citet{survey-feature-selection-extraction} analiza un conjunto de técnicas para la selección y extracción de atributos, con el propósito de determinar qué tan efectivas son estas técnicas a la hora de lograr un alto desempeño en clasificación.
Se presentan métodos de selección de atributos como mecanismos para la obtención de conjuntos de datos de menor dimensión.
Los métodos de selección de atributos pueden ser caracterizados por las siguientes etapas: búsquedas sobre los datos, generación de subconjuntos de datos y medida de la mejora en el subconjunto de datos.
Por otro lado, la extracción de atributos se presenta como la generación de nuevas características para reducir la complejidad y dar una representación más simple de los datos, expresando cada variable en el espacio de características nuevas como una combinación lineal de las variables originales.
Experimentalmente, se estudiaron siete conjuntos de datos médicos que incluían información sobre enfermedades oncológicas.
Los resultados obtenidos mostraron que el uso de técnicas de extracción de características permite obtener una mejor precisión que el uso de selección de características.
En particular, se ubicó a PCA (\textit{Principal Component Analysis}) como el método de extracción de características más utilizado.
PCA es un método no paramétrico simple que consiste en una transformación lineal de los datos que minimiza la redundancia, medida como la covarianza, maximizando la información, medida como la varianza.
Este método tiene las siguientes limitaciones: asume que la relación entre las variables es lineal, su interpretación de los datos resulta razonable únicamente si se asume que todas las variables están escaladas numéricamente y carece de un modelo probabilístico.
Para sobrellevar las dos primeras limitaciones se propuso el método PCA no lineal, donde las variables son observadas como categóricas.
Para afrontar la última limitación se propuso el método PPCA (\textit{Probabilistic Principal Component Analysis}), en el que PCA es uno de los parámetros de un modelo probabilístico. 
El trabajo de \citet{survey-feature-selection-extraction} fue un gran aporte a la hora de determinar cómo llevar a cabo la selección y/o extracción de atributos para la representación del problema estudiado en este proyecto de grado.

% ############### RESEÑA ###############

\section{MapReduce} \label{section-trabajos-relacionados-mapreduce}

\paragraph{}En el trabajo de \citet{mapreduce} se presenta una introducción al modelo MapReduce, así como detalles de su implementación y de su rendimiento.
Mapreduce es un modelo de programación utilizado para procesar y generar grandes conjuntos de datos.
En términos genéricos, un sistema implementado bajo el modelo MapReduce recibe datos en forma de un conjunto de pares (clave, valor), los transforma mediante la aplicación de una función definida por el usuario y devuelve un conjunto de pares (clave, valor) de salida agregados de acuerdo a un criterio también definido por el usuario.
La transformación aplicada a los datos en la entrada del sistema se llama \textit{map} y la transformación aplicada a los datos en la salida del sistema se llama \textit{reduce}.
En la implementación propuesta en Dean et al.
(2008), la computación dada en las funciones \textit{map} y \textit{reduce} es paralelizada automáticamente en múltiples recursos de cómputo, posibilitando el procesamiento masivo y paralelo de datos.
En concreto, el sistema presentado se encarga de inicializar múltiples hilos de ejecución en un régimen maestro-esclavo, donde el hilo maestro se encarga de asignar tareas \textit{map} o \textit{reduce} a los hilos esclavos.
De esta manera, se cuenta con \textit{map workers} y \textit{reduce workers}.
Cada \textit{map worker} recibe una porción de los datos, los procesa y guarda su salida en un disco local.
Cada \textit{reduce worker} recibe una ubicación (como por ejemplo una referencia a otra máquina de la red en un cluster) de donde deberá leer los datos intermedios generados por los \textit{map workers}, procesarlos y guardar su salida en un archivo.
Cuando todos los hilos finalizan su ejecución, el hilo \textit{master} retoma el control y reanuda la ejecución del programa de usuario que haya desencadenado la ejecución del MapReduce.
En el trabajo de \citet{mapreduce} también se mencionan técnicas utilizadas para el control de fallas, balanceo de carga y detalles específicos de implementación.
Además, se muestra una serie de resultados experimentales que consisten en una evaluación del rendimiento del modelo ejecutado en un cluster de 1800 máquinas de iguales características, en dos tipos de computación: búsqueda de un patrón en un terabyte de datos y ordenamiento de un terabyte de datos. 
El valor de este trabajo en relación al proyecto de grado reside en el hecho de que SV propone generar programas concurrentes automáticamente y la utilización de MapReduce puede resultar útil para posibilitar el paralelismo en dichos programas.

\section{Resumen}

\paragraph{}La Tabla \ref{table:trabajos-relacionados} presenta un resumen de los trabajos relacionados, manteniendo el orden en el que fueron mencionados en las secciones previas.
Se indica el autor, el año de publicación del trabajo y una breve descripción con los conceptos clave del mismo.

% ############### TABLA ###############
\begin{table}[htb]
\centering
\begin{tabular}{P{0.25\linewidth} P{0.05\linewidth} P{0.55\linewidth} } 
\specialrule{.2em}{.1em}{.1em} 
\multicolumn{1}{c}{Autores} & \multicolumn{1}{c}{Año}  & \multicolumn{1}{c}{Conceptos clave}\\ 
\hline
Dorronsoro et al.
\cite{savant-original} & 2013 & Se presenta el paradigma Savant para la paralelización de una heurística de planificación con aprendizaje automático utilizando SVM \\ 
\hline
Dorronsoro et al.
\cite{savant-bag} & 2016 & Estudio del problema de la mochila bajo el paradigma Savant \\
\hline
Kotsiantis, S.
\cite{ml-survey} & 2007 & Compendio de las técnicas de clasificación más utilizadas en aprendizaje automático supervisado \\
\hline
Chandrashekar et al.
\cite{fs-survey} & 2014 & Estudio comparativo de técnicas de selección de atributos en aprendizaje automático \\
\hline
Khalid et al.
\cite{survey-feature-selection-extraction} & 2014 & Compendio de técnicas de selección de atributos en aprendizaje automático, con foco en PCA \\
\hline
Dean et al.
\cite{mapreduce} & 2008 & Utilización de MapReduce para procesamiento de grandes volúmenes de datos en clusters \\
\specialrule{.2em}{.1em}{.1em} 
\end{tabular}
\caption{Trabajos relacionados al proyecto de grado}
\label{table:trabajos-relacionados}
\end{table}
% ############### FIN TABLA ###############

El análisis de la literatura relacionada muestra que el paradigma de SV en particular no ha sido estudiado de manera extensiva y permite llegar a la conclusión de que existe lugar para contribuir, por ejemplo profundizando en la utilización de métodos de aprendizaje automático distintos a SVM.

\newpage